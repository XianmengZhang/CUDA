>>>>>The GPU Computing Era(12 April 2010)
John Nickolls
William J. Dally
	Early GPUs had fixed functions, later on they were added with programmable flexibility
	CUDA is designed to be scalable
	Heterogeneous CPUþGPU coprocessing systems evolved because the CPU and GPU have complementary attributes that allow applications to perform best using both types of processors
	A heterogeneous coprocessing architecture that combines a single latency-optimized core (a CPU) with many throughputoptimized cores (a GPU) performs better than either alternative alone
	
>>>>>GPUs and the Future of Parallel Computing(17 October 2011)
Stephen W. Keckler
William J. Dally
Brucek Khailany
Michael Garland
David Glasco
	power supply voltage scaling has largely stopped, causing energy per operation to now scale only linearly with process feature size
	The result is that all computers, from mobile devices to supercomputers, have or will become constrained by power and energy rather than area
	CPU architecture were optimized for single-thread performance with features such as branch prediction, out-of-order execution, and large primary instruction and data caches. Most of the energy is consumed in overheads of data supply,instruction supply, and control
	(CPU)Today, the energy of a reasonable standard-cell-based, double-precision fused-multiply add (DFMA) is around 50 pJ, which would constitute less than 3 percent of the computational operation’s energy per instruction.
	 data movement will dominate future systems’ power dissipation
	 accessing DRAM cost 200 times greater than accessing SRAM
	 Current architectures encourage a flat view of memory with implicit data caching occurring at multiple levels of the machine,this approach is not sustainable for scalable machines
	 Processors can mitigate the effects of increased latency by relying on increasing amounts of parallelism, particularly finegrained thread and data parallelism (fetch data once and use it multiple times to avoid data bus congestion)
	 However, most programming systems don’t provide a convenient means of managing tens, much less tens of thousands, of threads on a single die
	 Future machines will be increasingly heterogeneous. Individual processor chips will likely contain processing elements with different performance characteristics, memory hierarchy views, and levels of physical parallelism
	 Nvidia Research team has embarked on the Echelon project to develop computing architectures that address energy-efficiency and memory-bandwidth challenges and provide features that facilitate programming of scalable parallel systems
	 
	 
>>>>>Dynamic Warp Formation and Scheduling for Efficient GPU Control Flow(1-5 Dec. 2007)
Wilson W.L. Fung
Ivan Sham
George Yuan
Tor M. Aamodt
	naive handling of branch divergence incurs a significant performance penalty on the GPU for controlflow intensive applications relative to an ideal multiple-instruction, multiple-data architecture
	reconverging control flow of processing elements at the immediate post-dominator of the divergent branch is nearly optimal
	important to find better branch handling mechanisms
	branch divergence hazzard: When an input dependent branch leads to different control flow paths for different threads in a warp, because an SIMD pipeline cannot execute different instructions in the same cycle
	naive solution to handle branch divergence: serialize the threads within a warp as soon as the program counters diverge, reconvergence mechanism needs to be used in order to prevent wasting resources
	immediate post-dominator: this is the point that is recommended to converge, usually
	Example of iPDOM is not the best: 3.3 not sure why ?????
	Dynamic Warp Formation & Scheduling: take partial threads from different warp (that has divergence in it) to form a new warp so every clockcycle a warp launched will be full
	lane aware dynamic warp formation: A better way to put multiple threads form different warps into a new warp and avoid conflict, based on each thread's original position in the each warp
	thread swizzling: when created the first warp, land all threads in a way purposely so that it can fuse with other warps easily while maintain its "lane" position (101010)(010101) => (111111)(000000)
	dynamic warp formation has the potential to fully utilize the SIMD pipeline only when the set of PC values currently being executed is small relative to the number of scalar threads (lots of thread doing little branch)
	Heuristics:
		controlling each thread to have similar depth of branching is optimal, becuase if all threads have different depth, it is hard to do dynamic warp formation (different PC)
		Majority (DMaj):			Choosing the most common PC among all the existing warps and issuing all warps at this PC before choosing a new PC
		Minority (DMin):			The least frequent PCs are given priority with the hope that, by doing so, these warps may eventually catch up and converge with other threads
		Time Stamp (DTime):			The oldest warp will be issued first
		Post-Dominator Priority (DPdPri):	If the issue priority is set lower for warps that have gone beyond more post-dominators, then the threads that have yet to go past the post-dominator tend to catch up
		Program Counter (DPC):			By giving higher issuing priority to warps with smaller PCs, threads lagging behind are given the opportunity to catch up
		
		
>>>>>Improving GPU Performance via Large Warps and Two-Level Warp Scheduling(December 3-7, 2011)
Veynu Narasiman
Michael Shebanow
Chang Joo Lee
Rustam Miftakhutdinov
Onur Mutlu
Yale N. Patt
	Our goal is to improve GPU performance by better utilizing computational resources. To alleviate the performance penalty due to branch divergence, we propose the large warp microarchitecture(LWM).
	creating fewer but correspondingly larger warps (that have a significantly larger number of threads than the SIMD width of the core), and dynamically creating SIMD width sized sub-warps that only have active threads (during divergence in PC)
	
	To reduce the number of idle FU (functional unit / core) cycles, applieda two-level round-robin warp instruction fetch scheduling policy on top of conventional GPU cores as well as the LWM
	This policy splits all concurrently executing warps into fetch groups (e.g., 32 warps could be split up into 4 fetch groups of 8 warps each) and prioritizes warps from a single fetch group until they reach a stalling point (i.e., long latency operation).  Then, the next fetch group is selected and the policy repeats. Warps in the same fetch group can be covered by locality ????
	The scheduling policy within a fetch group is round-robin, and switching from one fetch group to another is also done in a round-robin fashion (hence two-level round-robin).
	Once a warp is selected in the fetch stage, it cannot be selected again until that warp completes execution.
	Once a warp reaches the final stage of the pipeline, its PC and active mask are updated and the warp is again considered for scheduling
