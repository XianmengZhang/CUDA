>>>>>The GPU Computing Era(12 April 2010)
John Nickolls
William J. Dally
	Early GPUs had fixed functions, later on they were added with programmable flexibility
	CUDA is designed to be scalable
	Heterogeneous CPUþGPU coprocessing systems evolved because the CPU and GPU have complementary attributes that allow applications to perform best using both types of processors
	A heterogeneous coprocessing architecture that combines a single latency-optimized core (a CPU) with many throughputoptimized cores (a GPU) performs better than either alternative alone


>>>>>GPUs and the Future of Parallel Computing(17 October 2011)
Stephen W. Keckler
William J. Dally
Brucek Khailany
Michael Garland
David Glasco
	power supply voltage scaling has largely stopped, causing energy per operation to now scale only linearly with process feature size
	The result is that all computers, from mobile devices to supercomputers, have or will become constrained by power and energy rather than area
	CPU architecture were optimized for single-thread performance with features such as branch prediction, out-of-order execution, and large primary instruction and data caches. Most of the energy is consumed in overheads of data supply,instruction supply, and control
	(CPU)Today, the energy of a reasonable standard-cell-based, double-precision fused-multiply add (DFMA) is around 50 pJ, which would constitute less than 3 percent of the computational operation’s energy per instruction.
	 data movement will dominate future systems’ power dissipation
	 accessing DRAM cost 200 times greater than accessing SRAM
	 Current architectures encourage a flat view of memory with implicit data caching occurring at multiple levels of the machine,this approach is not sustainable for scalable machines
	 Processors can mitigate the effects of increased latency by relying on increasing amounts of parallelism, particularly finegrained thread and data parallelism (fetch data once and use it multiple times to avoid data bus congestion)
	 However, most programming systems don’t provide a convenient means of managing tens, much less tens of thousands, of threads on a single die
	 Future machines will be increasingly heterogeneous. Individual processor chips will likely contain processing elements with different performance characteristics, memory hierarchy views, and levels of physical parallelism
	 Nvidia Research team has embarked on the Echelon project to develop computing architectures that address energy-efficiency and memory-bandwidth challenges and provide features that facilitate programming of scalable parallel systems
	 
	 
>>>>>Dynamic Warp Formation and Scheduling for Efficient GPU Control Flow(1-5 Dec. 2007)
Wilson W.L. Fung
Ivan Sham
George Yuan
Tor M. Aamodt
	naive handling of branch divergence incurs a significant performance penalty on the GPU for controlflow intensive applications relative to an ideal multiple-instruction, multiple-data architecture
	reconverging control flow of processing elements at the immediate post-dominator of the divergent branch is nearly optimal
	important to find better branch handling mechanisms
	branch divergence hazzard: When an input dependent branch leads to different control flow paths for different threads in a warp, because an SIMD pipeline cannot execute different instructions in the same cycle
	naive solution to handle branch divergence: serialize the threads within a warp as soon as the program counters diverge, reconvergence mechanism needs to be used in order to prevent wasting resources
	immediate post-dominator: this is the point that is recommended to converge, usually
	Example of iPDOM is not the best: 3.3 not sure why ?????
	Dynamic Warp Formation & Scheduling: take partial threads from different warp (that has divergence in it) to form a new warp so every clockcycle a warp launched will be full
	lane aware dynamic warp formation: A better way to put multiple threads form different warps into a new warp and avoid conflict, based on each thread's original position in the each warp
	thread swizzling: when created the first warp, land all threads in a way purposely so that it can fuse with other warps easily while maintain its "lane" position (101010)(010101) => (111111)(000000)
	dynamic warp formation has the potential to fully utilize the SIMD pipeline only when the set of PC values currently being executed is small relative to the number of scalar threads (lots of thread doing little branch)
	Heuristics:
		controlling each thread to have similar depth of branching is optimal, becuase if all threads have different depth, it is hard to do dynamic warp formation (different PC)
		Majority (DMaj):			Choosing the most common PC among all the existing warps and issuing all warps at this PC before choosing a new PC
		Minority (DMin):			The least frequent PCs are given priority with the hope that, by doing so, these warps may eventually catch up and converge with other threads
		Time Stamp (DTime):			The oldest warp will be issued first
		Post-Dominator Priority (DPdPri):	If the issue priority is set lower for warps that have gone beyond more post-dominators, then the threads that have yet to go past the post-dominator tend to catch up
		Program Counter (DPC):			By giving higher issuing priority to warps with smaller PCs, threads lagging behind are given the opportunity to catch up


>>>>>Improving GPU Performance via Large Warps and Two-Level Warp Scheduling (December 3-7, 2011)
Veynu Narasiman
Michael Shebanow
Chang Joo Lee
Rustam Miftakhutdinov
Onur Mutlu
Yale N. Patt
	Our goal is to improve GPU performance by better utilizing computational resources. To alleviate the performance penalty due to branch divergence, we propose the large warp microarchitecture(LWM).
	creating fewer but correspondingly larger warps (that have a significantly larger number of threads than the SIMD width of the core), and dynamically creating SIMD width sized sub-warps that only have active threads (during divergence in PC)
	To improve long latency tolerance, we propose a two-level round-robin warp scheduling policy. This policy prevents all warps from arriving at the same long latency operation at the same time, thereby reducing idle execution cycles. (also preserving the locality as well)
	
	
	This policy splits all concurrently executing warps into fetch groups (e.g., 32 warps could be split up into 4 fetch groups of 8 warps each) and prioritizes warps from a single fetch group until they reach a stalling point (i.e., long latency operation).  Then, the next fetch group is selected and the policy repeats. Warps in the same fetch group can be covered by locality ????
	The scheduling policy within a fetch group is round-robin, and switching from one fetch group to another is also done in a round-robin fashion (hence two-level round-robin)
	Once a warp is selected in the fetch stage, it cannot be selected again until that warp completes execution.
	Once a warp reaches the final stage of the pipeline, its PC and active mask are updated and the warp is again considered for scheduling
	LWM with thread count of 256 works the best: if each thread of a large warp executes a loop for a different number of iterations, big warps size suffer because thread with small iterations is waiting for thread with large iterations (for loop + ifelse). On the other hand LWM with small thread count will suffer from not able to fully utilize dynamic warp formation.
	LWM with large thread thread count works fine with simple divergence (ifelse).
	fetch group size of 8 warps works best (two-level scheduling, with a fetch group size of 8)


>>>>>Cache-Conscious Wavefront Scheduling (1-5 Dec 2012)
Timothy G. Rogers
Mike OConnor
Tor M. Aamodt
	Intra-wavefront locality: locality that occurs when data is initially referenced and re-referenced from the same wavefront (warp)
	Intra-wavefront locality is a combination of intra-thread locality (where data is private to a single scalar thread) and inter-thread locality (where data is shared among scalar threads in the same wavefront)
	The majority of data reuse observed in our HCS benchmarks comes from intra-wavefront locality, Cache-Conscious Wavefront Scheduling is introduced to exploit it.
	CCWS alerts the scheduler if its decisions are destroying intra-wavefront locality (lost intra-wavefront locality detector (LLD))
	Correct amound of wavefronts can hide long latency operation and too many wavefront can destroy intra-wavefront locality (lots of misses)
	static wavefront limiting (SWL): current CUDA or OpenCL can define wavefront size, but GPU will run as many of them as possible. SWL is a register extension added to wavefront scheduling logic to limit the number of wavefronts that can be launched concurrently
	The goal of CCWS is to dynamically determine the number of wavefronts allowed to access the memory system and which wavefronts those should be
	Victim Tag Array (VTA) cannot be too large other wise it will keep track off every single cache miss and disrupt the scheduling ???
	simply increasing the capacity of the L1 cache only diminishes the performance impact of CCWS with small enough input sets. Hence, we believe CCWS will have an even greater impact on data sizes used in real workloads


>>>>>Energy-efficient mechanisms for managing thread context in throughput processors (4-8 June 2011)
Too many people
	Extreme multithreading requires a complicated thread scheduler as well as a large register file, whichis expensive to access both in terms of energy and latency
	
	register file caching to replace accesses to the large main register file (RFC MRF)
	The total register file capacity across the chip is 2MB, substantially exceeding the size of the L2 cache
	nearly 10% of total GPU power is consumed by the register file
	special write back procedure to prevent writing back "dead" data


	a two-level thread scheduler that maintains a small set of active threads to hide ALU and local memory access latency and a larger set of pending threads to hide main memory latency

	A two-level scheduler dramatically reduces the size of the RFC by allocating entries only to active warps


>>>>>Debunking the 100X GPU vs. CPU Myth:An Evaluation of Throughput Computing on CPU and GPU(June 19 - 23, 2010)
	include description of many CPU and GPU benchmark algorithms, covering a wide range of tasks
	limiting factors in both part:
		memory bandwith
		compute power
		cache policy
		gather and scatter methods (how to collect and store data to process)
		reduction and synchronization (barrier, thousands of cycles on CPU, 1 milisecond on GPU)
		fixed functionality (some features are fixed)
>>>>>GPU register file virtualization (December 05 - 09, 2015)
Hyeran Jeon
Gokul Subramanian Ravi
Nam Sung Kim
Murali Annavaram
	uses compiler-generated register lifetime information  to  proactively  release  registers  from one warp and opportunistically re-allocate that register to other warps
	
	Kepler relies on compiler to generate the dependency information and this information is conveyed to the hardware using metadata instructions that are embedded in the code
	A recent study found that one metadata instruction is added per seven instruction
	To enable register sharing across warps,  it is necessary to separate architected registers from the physical register space they occupy (allow multiple architected registers to share a single physical register)
	Per instruction release flag
	Per branch release flag
	Applications that have longer execution time derive higher register savings and hence our approach is particularly appealing to large kernels


>>>>>Shared Memory Consistency Models: A Tutorial (September 1995)
Sarita V. Adve
Kourosh Gharachorloo
	Sequential Consistency:
		the result of any execution is the same as if the operations of all the processors were executed in some sequential order
			(maintaining a single sequential order among operations from all processors)
		the operations of each individual processor appear in this sequence in the order specified by its program
			(maintaining program order among operations from individual processors)
	5.1	Sequential Consistency for architecture without caches (could have memory operation reordering)
		5.1.1
			maintaining program order between a write and a following read operation
				(some memory has write buffer but no read buffer, write finishes after read, even though write is issued after read)
		5.1.2	
			maintaining program order between two write operations
				(first write instruction must be executed first, write something and then write a flag)
		5.1.3
			maintaining program order between a read and a following read or write operation
				(first read instruction must be executed first, read flag and then decide what's next)
	5.2	Sequential Consistency for architecture with caches
		5.2.1
			a write is eventually made visible to all processors
			writes to the same location appear to be seen in the same order by all processors
