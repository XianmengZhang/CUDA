Chapter 3
P47 FIG 3.5	stub function vecAdd()
P50 FIG 3.7	CUDA API functions for managing device global memory
			cudaMalloc()
				((void**)&d_A, size)
			cudaFree()
				(d_A)
P51 FIG 3.8	CUDA API functions for data transfer between host and device
			cudaMemcpy()
				(d_A, A, size, cudaMemcpyHostToDevice)
				(C, d_C, size, cudaMemcpyDeviceToHost)
		ERROR HANDLING IN CUDA
			cudaError_t err = cudaMalloc((void**)&d_A, size);
P52 FIG 3.9	Example 3.7 3.8
P54 FIG 3.10	block and thread
			i = blockIdx.x * blockDim.x + threadIdx.x;
P55 FIG 3.11	kernel function (how to let each thread do its job)
			__global__
			void vecAddKernel(float* A, float* B, float* C, int n)
			{
			}
P55 FIG 3.12	Function declaration keyword:
			__device__
			__global__
			__host__
P57 FIG 3.13	How to call kernel function from stub function
			vecAddKernel<<<ceil(n/256.0), 256>>>(d_A, d_B, d_C, n);
			//<<< the number of blocks in the grid
			//the number of threads in each block >>>
P58 FIG 3.14	Example 3.7-3.13





Exercises 3.1a
	void 2DvecAdd(float* h_A, float* h_B, float* h_C, int I, int J)	// I is row count, J is column count
	{
		int size = I * J * sizeof(float);
		float * d_A,d_B,d_C;
		cudaMalloc((void **) &d_A, size);
		cudaMalloc((void **) &d_B, size);
		cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);
		cudaMalloc((void **) &d_C, size);
		cudaMemcpy(d_C, h_C, size, cudaMemcpyHostToDevice);
		2DvecAddKernel<<<ceil(I*J / 256.0), 256>>>(d_A, d_B, d_C, I, J); // assuming max thread 256
		cudaMemcpy(h_A, d_A, size, cudaMemcpyDeviceToHost);

		cudaFree(d_Ad); cudaFree(d_B); cudaFree (d_C);
	}
Exercises 3.1b
	__global__
	void 2DvecAddKernel(float* d_A, float* d_B, float* d_C, int I, int J)
	{
		int X = blockIdx.x * blockDim.x + threadIdx.x;
		if (X < I*J)	d_A[X]=d_B[X]+d_C[X];
	}
Exercises 3.1c	//	<<<ceil(I/256),256>>>
	__global__
	void 2DvecAddKernel(float* d_A, float* d_B, float* d_C, int I, int J)
	{
		int counter;
		
		int X = blockIdx.x * blockDim.x + threadIdx.x;
		if (X < I)
		{
			for(counter=0;counter<J;counter++)
			{
				d_A[X+counter]=d_B[X+counter]+d_C[X+counter];
			}
		}
	}
Exercises 3.1d	//	<<<ceil(J/256),256>>>
	__global__
	void 2DvecAddKernel(float* d_A, float* d_B, float* d_C, int I, int J)
	{
		int counter;
		
		int X = blockIdx.x * blockDim.x + threadIdx.x;
		if (X < J)
		for(counter=0;counter<I;counter++)
		{
			d_A[X+counter*J]=d_B[X+counter*J]+d_C[X+counter*J];
		}
	}
Exercises 3.1e
	from observation, rank efficiency of each solution: b>c>d. b is the best because it makes use of maximum amount of thread number and reduce the load of each thread. While d is the worst because it has the maximum overhead for each round of calculation for each thread, it needs to calculate the address of each column of values.







Exercises 3.2
	void vecMult(float* h_A, float* h_B, float* h_C, int SQ)//SQ = square
	{
		int sizeV =  SQ * sizeof(float);
		int sizeM =  SQ * SQ * sizeof(float);
		float * d_A,d_B,d_C;
		cudaMalloc((void **) &d_A, sizeV);
		cudaMalloc((void **) &d_B, sizeM);
		cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);
		cudaMalloc((void **) &d_C, sizeV);
		cudaMemcpy(d_C, h_C, size, cudaMemcpyHostToDevice);
		vecMultKernel<<<ceil(SQ / 256.0), 256>>>(d_A, d_B, d_C, I, J); // assuming max thread 256
		cudaMemcpy(h_A, d_A, size, cudaMemcpyDeviceToHost);

		cudaFree(d_Ad); cudaFree(d_B); cudaFree (d_C);
	}
	__global__
	void vecMultKernel(float* d_A, float* d_B, float* d_C, int SQ)
	{
		int X = blockIdx.x * blockDim.x + threadIdx.x;
		if (X < SQ)
		{
			for(int Y = 0; Y < SQ ; Y++)
			{
				d_A[X]=d_B[X*SQ + Y]+d_C[X];
			}
		}
	}
Exercises 3.3
	The purpose of host functions in most cases are to set up for device functions to execute
	The execution heavy host function defeats the purpose of CUDA

Exercises 3.4
	???
Exercises 3.5
	C
Exercises 3.6
	C
Exercises 3.7
	C













Chapter 4
P66		gridDim blockDim blockIdx
		gridDim.xyz = [1,65536]
		blockIdx.xyz = [0,gridDim.xyz-1]
P69		dim3 dimBlock(ceil(n/16.0), ceil(m/16.0), 1);	// how many blocks in each grid
		dim3 dimGrid(16, 16, 1);			// how many threads in each blocks
		pictureKernel <<< dimGrid, dimBlock >>> (d_Pin, d_Pout, n, m);
P72 FIG 4.4	kernel function for 2D thread mapping
		int Row = blockIdx.y * blockDim.y + threadIdx.y
P73 FIG 4.5	2D threading on processing image pixels
P76 FIG 4.6	matrix multiplication
P77 FIG 4.7	Example 4.6, kernel code
P77		compile-time constant
		autotuning
P78 FIG 4.8	Example 4.6, host code
		16*16 blocks in a grid
		64*64 threads in a block
P81		CUDA allows threads in the same block to coordinate their
activities using a barrier synchronization function __syncthreads()
P82 FIG 4.11	barrier synchronization
		__syncthreads() must be executed by all threads in the same block and through the same path (if else statement)
P83		threads cannot barrier synchronize others in different blocks
P84 FIG 4.12	transparent scalability
		streaming muliprocessors
P86		cudaGetDeviceCount()
		cudaGetDeviceProperties()
		dev_prop.maxThreadsPerBlock
		dev_prop.multiProcessorCount
		dev_prop.clockRate
		dev_prop.maxThreadsDim[012]	// threads in xyz dimension in each block
		dev_prop.maxGridSize[012]	// blocks in xyz dimension in each grid
P88		dev_prop.warpSize		// size of warps in a device
P89 FIG 4.14	SM, SP and Warp relations
		latency tolerance / latency hiding
P90		zero-overhead thread scheduling

Exercises 4.1
	C
Exercises 4.2
	C
Exercises 4.3
	B // assuming optimization will leave 2 warps (0/32,16/32) majorly empty instead of 4 half-full warps (20/32 * 4)
Exercises 4.4
	Maximum number of resident grids per device: 16
	Maximum number of threads per block: 1024
	Warp size: 32
	Maximum number of resident blocks per multiprocessor: 16
	Maximum number of resident warps per multiprocessor: 64
	Maximum number of resident threads per multiprocessor: 2048
	Assuming to maximize warp utility
	1024 * 351 = 359424 (576 short of 400 * 900)
	352 blocks needed, last block will have 576 thread left which is divisible by 32
	Not sure how to optimize since we do not know the total amount of devices
	each grid will have 11*2*1 = 22 blocks
	each blocks will have 8*8*16 = 1024 threads
Exercises 4.5
	576
Exercises 4.6
	17.083%
Exercises 4.7
	lacking enough information
Exercises 4.8
	No, all thread in the same block should have similar logic pattern for each instance of barrier synchronization. It would be wasting processing power if threads in the same block takes very different time to reach barrier.
Exercises 4.9
	I will be surprised because his multiplication code uses 1024 threads in each block, while the CUDA device only allows 512 threads per block and 8 blocks per SM
Exercises 4.10.aa
	BLOCK_SIZE = BLOCK_WIDTH ^ 2 (might misunderstood the question)
	BLOCK_SIZE = 1 (transpose it self will always work)
Exercises 4.10.b
	Insert a barrier synchronization before the last line of code so blockA array will be completed before it is referenced.




Chapter 5
P96		compute to global memory access ratio (higher the better)
P98 FIG 5.2	CUDA device memory map
			register
			local memory
			shared memory
			global memory	(DRAM)
			constant memory
P101		Scratchpad memory
P102 TAB 5.1	CUDA variables: Automatic,__device__,__shared__, __constant__
P111		global memory access is reduced by N if the tasked is tiled with N*N tiles
P112 FIG 5.12	tile method to utilize locality

P116		dev_prop.regsPerBlock gives the number of registers available in each
SM (through cudaGetDeviceProperties())
P117		dev_prop.sharedMemPerBlock
P118		Use extern to change your CUDA declaration easily
Exercises 5.1
	No, all the data were used once therefore not exhibiting locality characteristic 
Exercises 5.2
	As tile size increases, the size of each tile row will increase proportionally, in the case of matrix multiplication, each row of a matrix will be used and hence the global memory access time will be reduced proportionally to the tile size growth.
Exercises 5.3
	some thread finishes early will try to load the next tile, while slower thread will try to access the current tile, this results in shared memory keep loading and unloading its current space with global memory
Exercises 5.4
	In a case where the result from on block needs to be processed by another block, it would make more sense to store the result in shared memory instead of first block's registers. Extra memory relocation will be needed in shared memory if the second block wants to read first block's register.
Exercises 5.5
	C
Exercises 5.6
	D
Exercises 5.7
	B
Exercises 5.8	???
	the shared memory can be accessed by any thread in the block while L1 cache can only be accessed by a single processor
Exercises 5.9a	???
	N
Exercises 5.9b	???
	N/T
Exercises 5.10a
	36/200 < 7*4/100, memory bound
Exercises 5.10b
	36/300 > 7*4/250, compute bound
Exercises 5.11
	??? cannot find what compute capability is, except for the documentation on Nvidia webpage

Chapter 6
P127		control flow
		diverge (in their execution)
P128		reduction algorithm (split large task into multiple small tasks)
P129 FIG 6.2	example kernel code for sum reduction
P131 FIG 6.4	example kernel code for sum reduction,improved
P134		DRAM and coalesces
P136 FIG 6.8	coalesced access
P138 FIG 6.9	not coalesced access
P139 FIG 6.10	shared memory can help coalescing
P140 FIG 6.11	example kernel code for  FIG 6.6~6.10
P141		accessing arrays in shared memory does not require coalescing to achieve high-speed
P143		performance cliff
Exercises 6.1
	// for "/2", some corner cases was simplified
	// still not quite sure how the thread count can be reduced
	///////////////////////////////
	__shared__ float partialSum[]
	unsigned int t = threadIdx.x*2;
	if (t <= threadIdx.x)
	{
		for (unsigned int stride = 1; stride < blockDim.x; stride *= 2)
		{
			__syncthreads();
			if (t % (2*stride) == 0)
			partialSum[t] += partialSum[t+stride];
		}
	}
	///////////////////////////////
	__shared__ float partialSum[];
	unsigned int t = threadIdx.x/2;
	for (unsigned int stride = blockDim.x/2; stride > 1; stride /= 2)
	{
		__syncthreads();
		if (t < stride)
		partialSum[t] += partialSum[t+stride];
	}
	Yes, but very little
	Memory bus conflict and thread/block distribution problem
Exercises 6.2
	the second method introduce less operations
Exercises 6.3
(1) adding the statements that load a section of the input array from global memory to shared memory
(2) using blockIdx.x to allow multiple blocks to work on different sections of the input array
(3) writing the reduction value for the section to a location according to the blockIdx.x so that all blocks will deposit their section reduction value to the lower part of the input array in global memory.
	__global__
	void sumReductionKernel(float* d_A)
	{
		__shared__ float s_A[blockDim.x];
		for (int m = 0; m < blockDim.x; ++m)
		{
			s_A[m] = d_A[blockDim.x*blockIdx.x + m];
		}

		unsigned int t = threadIdx.x; // assuming thread numbers is already set as half of size of s_A

		for (unsigned int stride = blockDim.x/2; stride > 1; stride /= 2)
		{
			__syncthreads();
			if (t < stride)
			{
				s_A[t] += s_A[t+stride];
			}
		}
		d_A[blockIdx.x] = s_A[0] + s_A[1];
	}
Exercises 6.4
(1) transfer a large input array to the global memory
(2) use a loop to repeatedly invoke the kernel you wrote for Exercise 6.3 with adjusted execution configuration parameter values so that the reduction result for the input array will eventually be produced.

	void sumReduction(float* h_A, int size)	// size is the number of elements in 1D array h_A
	{
		float * d_A;

		cudaMalloc((void **) &d_A, size);
		cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
		sumReductionKernel<<<ceil(n / 256.0), 256>>>(d_A); // assuming 256 threads
		cudaMemcpy(h_A, d_A, size, cudaMemcpyDeviceToHost);

		cudaFree(d_A);
	}

			vecAddKernel<<<ceil(n/256.0), 256>>>(d_A, d_B, d_C, n);
			//<<< the number of blocks in the grid
			//the number of threads in each block >>>
